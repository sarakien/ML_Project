---
title: "Machine Learning Project"
output: html_document

---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

```

Read Data into R and create train and test sets.

```{r}
library(dplyr)
library(randomForest)

final_data = read.csv("final_data.csv")

set.seed(0)
train = sample(1:nrow(final_data), 8*nrow(final_data)/10)
ames.train = final_data[train, ]
ames.test = final_data[-train, ]

```

Model a Forest:

```{r}

set.seed(0)
rf.default = randomForest(SalePrice ~ ., data = ames.train, importance = TRUE)
rf.default #% Var explained = 87.23%

#Varying the number of variables used at each step of the random forest procedure.
set.seed(0)
oob.err = numeric(77)
for (mtry in 1:77) {
  fit = randomForest(SalePrice ~ ., data = ames.train, mtry = mtry)
  oob.err[mtry] = fit$mse[500]
  cat("We're performing iteration", mtry, "\n")
}

#Visualizing the OOB error.
plot(1:77, oob.err, pch = 16, type = "b",
     xlab = "Variables Considered at Each Split",
     ylab = "OOB Mean Squared Error",
     main = "Random Forest OOB Error Rates\nby # of Variables")

# top variables
importance(rf.default)
varImpPlot(rf.default)
#The GrLivArea and OverallQual are driving the largest mean decrease
#in MSE and greatest increase in purity, meaning that they are the most important predictors in the current random forest.

```
Exploring different ntrees


```{r}

set.seed(0)
rf.one = randomForest(SalePrice ~ ., data = ames.train, importance = TRUE, ntree = 100)
rf.one #86.45 var explained

set.seed(0)
rf.two = randomForest(SalePrice ~ ., data = ames.train, importance = TRUE, ntree = 200)
rf.two #86.7 var explained

set.seed(0)
rf.three = randomForest(SalePrice ~ ., data = ames.train, importance = TRUE, ntree = 300)
rf.three #87.18 var explained

set.seed(0)
rf.six = randomForest(SalePrice ~ ., data = ames.train, importance = TRUE, ntree = 600)
rf.six #87.53 var explained
importance(rf.six)
varImpPlot(rf.six)

set.seed(0)
rf.seven = randomForest(SalePrice ~ ., data = ames.train, importance = TRUE, ntree = 700)
rf.seven #87.4 var explained


#Parameters of # of vars = 25 and ntree = 600 are ideal, with 87.53% of variance exlplained

```



Predict the test data and visualize

```{r}

predictions = predict(rf.six, ames.test)
predictions

result = ames.test
result['prediction'] = predictions

head(result)

# Import library for visualization
library(ggplot2)

# Build scatterplot
ggplot(  ) + 
  geom_point( aes(x = result$SalePrice, y = result$prediction, color = 'red') ) + 
  labs(x = "SalePrice", y = "Prediction", color = "")

cor.test(result$SalePrice, result$prediction, method=c("pearson")) #r = .95

```

Examine MAE, MSE, and R-Squared scores of predictions

```{r}

install.packages('Metrics')
library(Metrics)

print(paste0('MAE: ' , mae(result$SalePrice,predictions) )) 
#MAE = 15587.81

print(paste0('MSE: ' ,caret::postResample(predictions , result$SalePrice)['RMSE']^2 )) 
#MSE = 528564768.11

print(paste0('R2: ' ,caret::postResample(predictions , result$SalePrice)['Rsquared'] ))
#R^2 = 0.90
```


Conduct gradient boosting and compare. *NOTE: Need to factorize categorical variables first (did that in python). I also Box Cox transformed SalePrice as a preliminary step toward dealing with outliers.

```{r}

library(gbm)

encoded_dataT = read.csv("encoded_dataT.csv")

encoded.train = encoded_dataT[train, ]
encoded.test = encoded_dataT[-train, ]

dim(encoded.test)

#Fitting 10,000 trees with a depth of 4.
set.seed(0)
boost.houses = gbm(SalePrice ~ ., data = encoded.train,
                   distribution = "gaussian",
                   n.trees = 10000,
                   interaction.depth = 4)

#Inspecting the relative influence.
par(mfrow = c(1, 1))
summary(boost.houses)

```

Vary shrinkage. 

```{r}

#Letâ€™s make a prediction on the test set. With boosting, the number of trees is
#a tuning parameter; having too many can cause overfitting. In general, we should
#use cross validation to select the number of trees. Instead, we will compute the
#test error as a function of the number of trees and make a plot for illustrative
#purposes.
n.trees = seq(from = 100, to = 10000, by = 100)
predmat = predict(boost.houses, newdata = encoded.test, n.trees = n.trees)

#Produces 100 different predictions for each of the 291 observations in our
#test set.
dim(predmat)

#Calculating the boosted errors.
par(mfrow = c(1, 1))
berr = with(encoded.test, apply((predmat - SalePrice)^2, 2, mean))
plot(n.trees, berr, pch = 16,
     ylab = "Mean Squared Error",
     xlab = "# Trees",
     main = "Boosting Test Error")


#Increasing the shrinkage parameter; a higher proportion of the errors are
#carried over.
set.seed(0)
boost.houses2 = gbm(SalePrice ~ ., data = encoded.train,
                    distribution = "gaussian",
                    n.trees = 10000,
                    interaction.depth = 4,
                    shrinkage = 0.04)

#Inspecting the relative influence.
par(mfrow = c(1, 1))
summary(boost.houses2)

predmat2 = predict(boost.houses2, newdata = encoded.test, n.trees = n.trees)

mean((predmat2-encoded.test$SalePrice)^2) 

berr2 = with(encoded.test, apply((predmat2 - SalePrice)^2, 2, mean))
plot(n.trees, berr2, pch = 16,
     ylab = "Mean Squared Error",
     xlab = "# Trees",
     main = "Boosting Test Error")

#Lambda of 0.04 appears to produce the best learning rate.
```


Run model with best parameters based on Boosting Test Error plot to examine ideal set of feature importances, which will inform subset selection for Multiple Linear Regression.

```{r}

set.seed(0)
boost.houses4 = gbm(SalePrice ~ ., data = encoded.train,
                   distribution = "gaussian",
                   n.trees = 500,
                   interaction.depth = 4,
                   shrinkage = 0.04)

#Inspecting the relative influence.
par(mfrow = c(1, 1))
summary(boost.houses4)

predmat4 = predict(boost.houses4, newdata = encoded.test, n.trees = 500)

mean((predmat4-encoded.test$SalePrice)^2) #RSS = 0.002


```

Use results from Random Forest and Gradient Boosting to inform Subset Selection in Multiple Regression.

Import data prepped for MLR in Python.

Examine multicollinearity.


```{r}

mlr_data = read.csv("MLR_data.csv")
head(mlr_data)

library(car)

model.prelim = lm(SalePriceT ~ ., data = mlr_data)
summary(model.prelim)
plot(model.prelim)
vif(model.prelim)
avPlots(model.prelim)

mlr_data_nmc = subset(mlr_data, select = c(-GrLivAreaT,-YearBuiltT,-UnFinBsmtCode))

head(mlr_data_nmc)

model.prelim.nmc = lm(SalePriceT ~ ., data = mlr_data_nmc)
summary(model.prelim.nmc)
plot(model.prelim.nmc)
vif(model.prelim.nmc)
avPlots(model.prelim.nmc)


anova(model.prelim.nmc, model.prelim) 

AIC(model.prelim, model.prelim.nmc)
BIC(model.prelim, model.prelim.nmc)

#According to the anova and AIC/BIC analyses, model.prelim is the best model. However, there is substantial multicollinearity in that model. Therefore - will need to use penalization to address multicollinearity.


```

Explore potential interaction effects in Multiple Linear Regression. 

Compare variables across levels of neighborhood code.


```{r}

high.affluent = mlr_data %>% filter(., NeighborhoodCode == 3) 
high.affluent = subset(high.affluent, select = 
                         c(-NeighborhoodCode, -GarageCode, -CentralAirCode, -PavedDriveCode))
#latter three variables are singular in the high-affluent data set

mid.affluent = mlr_data %>% filter(., NeighborhoodCode == 2)
mid.affluent = subset(mid.affluent, select = -NeighborhoodCode)

low.affluent = mlr_data %>% filter(., NeighborhoodCode == 1)
low.affluent = subset(low.affluent, select = -NeighborhoodCode)

model.ha = lm(SalePriceT ~ ., data = high.affluent)
summary(model.ha)
plot(model.ha)
vif(model.ha)

model.ma = lm(SalePriceT ~ ., data = mid.affluent)
summary(model.ma)
plot(model.ma)
vif(model.ma)

model.la = lm(SalePriceT ~ ., data = low.affluent)
summary(model.la)
plot(model.la)
vif(model.la)


high.affluent


```

Investigate potential interactions with building type.

```{r}

single_family = mlr_data %>% filter(., BldgTypeCode == 1) 
single_family = subset(high.affluent, select = -BldgTypeCode)

attached = mlr_data %>% filter(., BldgTypeCode == 0) 
attached = subset(high.affluent, select = -BldgTypeCode)

model.sf = lm(SalePriceT ~ ., data = single_family)
summary(model.sf)
plot(model.sf)

model.att = lm(SalePriceT ~ ., data = attached)
summary(model.att)
plot(model.att)

#Note - models are identical with respect to variable importance

```


Create interaction terms in python for: 1) NeighborhoodCode x KitchenQualCode, 2) NeighborhoodCode x BathTot, 3) NeighborhoodCode x 1stFlrSF, and 4) NeighborhoodCode x MSVnrCode, add to data set and conduct multiple regression.

Import data from python that includes interaction terms. Conduct MLR and compare to previous model.

```{r}

mlr_data_int = read.csv("MLR_data_int_full.csv")
head(mlr_data_int)

model.int = lm(SalePriceT ~ ., data = mlr_data_int)
summary(model.int)
plot(model.int)
vif(model.int)
avPlots(model.int)


anova(model.prelim, model.int) 

AIC(model.prelim, model.int)
BIC(model.prelim, model.int)

#According to the AIC/BIC analyses, model.int is the best model. However, there is substantial multicollinearity in that model.

```

Ok well the model is better with all variables, even though there is a lot of multicollinearity.

Therefore -- have to penalize to control for multicollinearity using regularization and cross-validation.

https://www.datacamp.com/community/tutorials/tutorial-ridge-lasso-elastic-net
"Lasso tends to do well if there are a small number of significant parameters and the others are close to zero (ergo: when only a few predictors actually influence the response). Ridge works well if there are many large parameters of about the same value (ergo: when most predictors impact the response)."

Therefore I will proceed with Ridge.

```{r}
library(glmnet)

scaled.dat = as.data.frame(scale(mlr_data_int))
head(scaled.dat)

#Create matrices for glmnet() function and automatically conduct conversions
#for factor variables into dummy variables.
x = model.matrix(SalePriceT ~ ., scaled.dat)[, -1] #Dropping the intercept column.
y = scaled.dat$SalePriceT
head(x)

#Values of lambda over which to check.
grid = 10^seq(5, -2, length = 100) 
#NOTE: ^^ this selects a range of lambdas to check, starting at 10^5 and ending 
#at 10^2, separated by 100 steps/intervals (i.e., 100 lambdas)

#Creating training and testing sets. Here we decide to use a 70-30 split with
#approximately 70% of our data in the training set and 30% of our data in the
#test set.
set.seed(0)
train = sample(1:nrow(x), 7*nrow(x)/10)
test = (-train)
y.test = y[test]

length(train)/nrow(x)
length(y.test)/nrow(x)

#Fitting the ridge regression. Alpha = 0 for ridge regression.
#Running 10-fold cross validation.
set.seed(0)
cv.ridge.out = cv.glmnet(x[train, ], y[train],
                         lambda = grid, alpha = 0, nfolds = 10)
plot(cv.ridge.out, main = "Ridge Regression\n")
bestlambda.ridge = cv.ridge.out$lambda.min
bestlambda.ridge #0.01
log(bestlambda.ridge) #-4.61

#With "cv.ridge.out", we can actually access
#the best model from the cross validation without calling "ridge.models.train"
#or "bestlambda.ridge":
ridge.bestlambdatrain = predict(cv.ridge.out, s ="lambda.min", newx = x[test, ])
mean((ridge.bestlambdatrain - y.test)^2) #MSE is 0.10

ridge.best.lambda = glmnet(x, y, alpha = 0, lambda = .01)
ridge_coefs = coef(ridge.best.lambda) #Most estimates close to 0.
ridge_coefs = as.data.frame(as.matrix(ridge_coefs))
ridge_coefs$variables = rownames(ridge_coefs)
ridge_coefs_data = ridge_coefs[order(ridge_coefs$s0, decreasing = TRUE),]
ridge_coefs_data 
write.csv(ridge_coefs_data, file = 'ridge_coefs.csv')

```
